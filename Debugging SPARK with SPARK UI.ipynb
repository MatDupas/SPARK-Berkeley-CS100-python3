{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Download Illiad Odyssey\n",
    "# !wget http://classics.mit.edu/Homer/iliad.mb.txt\n",
    "# !wget http://classics.mit.edu/Homer/odyssey.mb.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(word):\n",
    "    from nltk.corpus import stopwords\n",
    "    english_stop_words = stopwords.words(\"english\")\n",
    "    return word not in english_stop_words\n",
    "\n",
    "def load_text(text_path):\n",
    "    # Split text in words\n",
    "    # Remove empty word artefacts\n",
    "    # Remove stop words ('I', 'you', 'a', 'the', ...)\n",
    "    vocabulary = sc.textFile(text_path)\\\n",
    "        .flatMap(lambda lines: lines.lower().split())\\\n",
    "        .flatMap(lambda word: word.split(\".\"))\\\n",
    "        .flatMap(lambda word: word.split(\",\"))\\\n",
    "        .flatMap(lambda word: word.split(\"!\"))\\\n",
    "        .flatMap(lambda word: word.split(\"?\"))\\\n",
    "        .flatMap(lambda word: word.split(\"'\"))\\\n",
    "        .flatMap(lambda word: word.split(\"\\\"\"))\\\n",
    "        .filter(lambda word: word is not None and len(word) > 0)\\\n",
    "        .filter(filter_stop_words)\n",
    "\n",
    "    # Count the total number of words in the text\n",
    "    word_count = vocabulary.count()\n",
    "\n",
    "    # Compute the frequency of each word: frequency = #appearances/#word_count\n",
    "    word_freq = vocabulary.map(lambda word: (word, 1))\\\n",
    "        .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "        .map(lambda tupl: (tupl[0], tupl[1]/float(word_count))) #(word, count) -> (word, count/float(word_count)\n",
    "\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.52 ulysses\n",
      "53.63 house\n",
      "48.33 telemachus\n",
      "43.06 suitors\n",
      "36.68 tell\n",
      "33.47 ship\n",
      "33.35 one\n",
      "31.94 home\n",
      "26.73 said\n",
      "25.97 got\n",
      "-28.72 jove\n",
      "-31.46 horses\n",
      "-40.66 fight\n",
      "-44.56 spear\n",
      "-47.24 ships\n",
      "-54.71 achilles\n",
      "-61.74 achaeans\n",
      "-65.52 hector\n",
      "-72.71 trojans\n",
      "-89.71 son\n",
      "Task Duration: 257.4s\n",
      "press ENTER to exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "tic= time.time()\n",
    "\n",
    "iliad = load_text('./iliad.mb.txt')\n",
    "odyssey = load_text('./odyssey.mb.txt')\n",
    "\n",
    "# Join the two datasets and compute the difference in frequency\n",
    "# Note that we need to write (freq or 0) because some words do not appear\n",
    "# in one of the two books. Thus, some frequencies are equal to None after\n",
    "# the full outer join.\n",
    "def freq_diff(tupl):\n",
    "    (word, (freq1, freq2)) = tupl\n",
    "    return (word, (freq2 or 0) - (freq1 or 0))\n",
    "\n",
    "\n",
    "\n",
    "join_words = iliad.fullOuterJoin(odyssey)\\\n",
    "     .map(lambda tupl: freq_diff(tupl))\n",
    "\n",
    "# 10 words that get a boost in frequency in the sequel\n",
    "emerging_words = join_words.takeOrdered(10, lambda rec: -rec[1])\n",
    "\n",
    "# 10 words that get a decrease in frequency in the sequel\n",
    "disappearing_words = join_words.takeOrdered(10, lambda rec: rec[1])\n",
    "\n",
    "# Print results\n",
    "for word, freq_diff in emerging_words:\n",
    "    print(\"%.2f\" % (freq_diff*10000), word)\n",
    "for word, freq_diff in disappearing_words[::-1]:\n",
    "    print(\"%.2f\" % (freq_diff*10000), word)\n",
    "\n",
    "tac= time.time()\n",
    "print(\"Task Duration: {}s\".format(round(tac-tic,1)))\n",
    "    \n",
    "# WAIT in order to inspect all jobs at http://localhost:4040/\n",
    "input(\"press ENTER to exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When inspecting Process in SPARK WEB UI, we can see that we step 2 and 3 take all the time. We performed actions 2 times, one during count and another during takeOrdered so RDD is computed twice : this can be avoided by using persistance (persist() or cache() operation )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_optimized(text_path):\n",
    "    # Split text in words\n",
    "    # Remove empty word artefacts\n",
    "    # Remove stop words ('I', 'you', 'a', 'the', ...)\n",
    "    vocabulary = sc.textFile(text_path)\\\n",
    "        .flatMap(lambda lines: lines.lower().split())\\\n",
    "        .flatMap(lambda word: word.split(\".\"))\\\n",
    "        .flatMap(lambda word: word.split(\",\"))\\\n",
    "        .flatMap(lambda word: word.split(\"!\"))\\\n",
    "        .flatMap(lambda word: word.split(\"?\"))\\\n",
    "        .flatMap(lambda word: word.split(\"'\"))\\\n",
    "        .flatMap(lambda word: word.split(\"\\\"\"))\\\n",
    "        .filter(lambda word: word is not None and len(word) > 0)\\\n",
    "        .filter(filter_stop_words).cache()\n",
    "\n",
    "    # Count the total number of words in the text\n",
    "    word_count = vocabulary.count()\n",
    "\n",
    "    # Compute the frequency of each word: frequency = #appearances/#word_count\n",
    "    word_freq = vocabulary.map(lambda word: (word, 1))\\\n",
    "        .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "        .map(lambda tupl: (tupl[0], tupl[1]/float(word_count))) #(word, count) -> (word, count/float(word_count)\n",
    "\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.52 ulysses\n",
      "53.63 house\n",
      "48.33 telemachus\n",
      "43.06 suitors\n",
      "36.68 tell\n",
      "33.47 ship\n",
      "33.35 one\n",
      "31.94 home\n",
      "26.73 said\n",
      "25.97 got\n",
      "-28.72 jove\n",
      "-31.46 horses\n",
      "-40.66 fight\n",
      "-44.56 spear\n",
      "-47.24 ships\n",
      "-54.71 achilles\n",
      "-61.74 achaeans\n",
      "-65.52 hector\n",
      "-72.71 trojans\n",
      "-89.71 son\n",
      "Task Duration: 145.0s\n",
      "press ENTER to exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "tic= time.time()\n",
    "\n",
    "iliad = load_text_optimized('./iliad.mb.txt')\n",
    "odyssey = load_text_optimized('./odyssey.mb.txt')\n",
    "\n",
    "# Join the two datasets and compute the difference in frequency\n",
    "# Note that we need to write (freq or 0) because some words do not appear\n",
    "# in one of the two books. Thus, some frequencies are equal to None after\n",
    "# the full outer join.\n",
    "def freq_diff(tupl):\n",
    "    (word, (freq1, freq2)) = tupl\n",
    "    return (word, (freq2 or 0) - (freq1 or 0))\n",
    "\n",
    "\n",
    "# Another Optimization : We cache join.words\n",
    "join_words = iliad.fullOuterJoin(odyssey)\\\n",
    "     .map(lambda tupl: freq_diff(tupl))\n",
    "\n",
    "# 10 words that get a boost in frequency in the sequel\n",
    "emerging_words = join_words.takeOrdered(10, lambda rec: -rec[1])\n",
    "\n",
    "# 10 words that get a decrease in frequency in the sequel\n",
    "disappearing_words = join_words.takeOrdered(10, lambda rec: rec[1])\n",
    "\n",
    "# Print results\n",
    "for word, freq_diff in emerging_words:\n",
    "    print(\"%.2f\" % (freq_diff*10000), word)\n",
    "for word, freq_diff in disappearing_words[::-1]:\n",
    "    print(\"%.2f\" % (freq_diff*10000), word)\n",
    "\n",
    "tac= time.time()\n",
    "print(\"Task Duration: {}s\".format(round(tac-tic,1)))\n",
    "    \n",
    "# WAIT in order to inspect all jobs at http://localhost:4040/\n",
    "input(\"press ENTER to exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.57976653696498"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(257-145)/257*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We gained 43% on time by just caching the intermediate results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also optimize the time used to look at the stop_words**, as they are defined as a list and program has to read them all to find the good one. \n",
    "\n",
    "**One solution is to use a Hash table (dictionary or set).\n",
    "Let's use a set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via hash table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New filter_stop_table function using hash table saved as variable\n",
    "from nltk.corpus import stopwords\n",
    "english_stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def filter_stop_words(word):\n",
    "    return word not in english_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.52 ulysses\n",
      "53.63 house\n",
      "48.33 telemachus\n",
      "43.06 suitors\n",
      "36.68 tell\n",
      "33.47 ship\n",
      "33.35 one\n",
      "31.94 home\n",
      "26.73 said\n",
      "25.97 got\n",
      "-28.72 jove\n",
      "-31.46 horses\n",
      "-40.66 fight\n",
      "-44.56 spear\n",
      "-47.24 ships\n",
      "-54.71 achilles\n",
      "-61.74 achaeans\n",
      "-65.52 hector\n",
      "-72.71 trojans\n",
      "-89.71 son\n",
      "Task Duration: 33.7s\n",
      "press ENTER to exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "tic= time.time()\n",
    "\n",
    "iliad = load_text_optimized('./iliad.mb.txt')\n",
    "odyssey = load_text_optimized('./odyssey.mb.txt')\n",
    "\n",
    "# Join the two datasets and compute the difference in frequency\n",
    "# Note that we need to write (freq or 0) because some words do not appear\n",
    "# in one of the two books. Thus, some frequencies are equal to None after\n",
    "# the full outer join.\n",
    "def freq_diff(tupl):\n",
    "    (word, (freq1, freq2)) = tupl\n",
    "    return (word, (freq2 or 0) - (freq1 or 0))\n",
    "\n",
    "\n",
    "# Another Optimization : We cache join.words\n",
    "join_words = iliad.fullOuterJoin(odyssey)\\\n",
    "     .map(lambda tupl: freq_diff(tupl))\n",
    "\n",
    "# 10 words that get a boost in frequency in the sequel\n",
    "emerging_words = join_words.takeOrdered(10, lambda rec: -rec[1])\n",
    "\n",
    "# 10 words that get a decrease in frequency in the sequel\n",
    "disappearing_words = join_words.takeOrdered(10, lambda rec: rec[1])\n",
    "\n",
    "# Print results\n",
    "for word, freq_diff in emerging_words:\n",
    "    print(\"%.2f\" % (freq_diff*10000), word)\n",
    "for word, freq_diff in disappearing_words[::-1]:\n",
    "    print(\"%.2f\" % (freq_diff*10000), word)\n",
    "\n",
    "tac= time.time()\n",
    "print(\"Task Duration: {}s\".format(round(tac-tic,1)))\n",
    "    \n",
    "# WAIT in order to inspect all jobs at http://localhost:4040/\n",
    "input(\"press ENTER to exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.15953307392996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(257-33)/257*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duration reached 33s ! We gain 87% of the time thanks to the hash table trick !** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
